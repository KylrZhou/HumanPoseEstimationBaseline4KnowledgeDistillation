{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab6375be-550e-4f96-946c-c04e96ffc1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Conv2d, BatchNorm2d, ReLU, MaxPool2d, Sequential, AdaptiveAvgPool2d, Linear, Softmax, CrossEntropyLoss, MSELoss, NLLLoss\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, RandomHorizontalFlip, RandomGrayscale, Resize, InterpolationMode\n",
    "from torch.cuda import memory_allocated, empty_cache, memory_reserved, IntTensor\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import Adam, SGD, lr_scheduler\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "973e3f1c-8497-41d3-9875-277921ac7003",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transform_Train = Compose([RandomHorizontalFlip(),\n",
    "                           RandomGrayscale(),\n",
    "                           ToTensor(),\n",
    "                           Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n",
    "Transform_Test = Compose([ToTensor(),\n",
    "                           Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n",
    "log_interval = 10\n",
    "momentum = 0.9\n",
    "TrainBS = 8\n",
    "TestBS = 64\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "190fce8e-83d5-41a9-be02-bcd021621d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('./log/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30cfedaf-8db0-4841-a41b-361e1b4f4e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "Train_Data = DataLoader(dataset = CIFAR10(root = './data/',\n",
    "                                          train = True,\n",
    "                                          transform = Transform_Train,\n",
    "                                          download = True),\n",
    "                        batch_size = TrainBS,\n",
    "                        shuffle = True)\n",
    "Test_Data = DataLoader(dataset = CIFAR10(root = './data/',\n",
    "                                          train = False,\n",
    "                                          transform = Transform_Test,\n",
    "                                          download = True),\n",
    "                        batch_size = TestBS,\n",
    "                        shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "499fc50c-c416-4ce8-bae2-e599a77ccb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, DownSample, Proc_Channel):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        stride = 1\n",
    "        self.shortcut = Sequential()\n",
    "        in_channels = Proc_Channel\n",
    "        if DownSample == 1:\n",
    "            if Proc_Channel != 64:\n",
    "                in_channels = int(Proc_Channel/2)\n",
    "                stride = 2\n",
    "            self.shortcut = Sequential(Conv2d(in_channels = in_channels, out_channels = Proc_Channel, kernel_size = 3, stride = stride, padding = 1),\n",
    "                                       BatchNorm2d(Proc_Channel))    \n",
    "        kernel_size = 3\n",
    "        self.ConvLayer1 = Conv2d(in_channels, Proc_Channel, kernel_size, stride, 1)\n",
    "        self.BatchNorm1 = BatchNorm2d(Proc_Channel)\n",
    "        self.ConvLayer2 = Conv2d(in_channels = Proc_Channel, out_channels = Proc_Channel, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.BatchNorm2 = BatchNorm2d(Proc_Channel)\n",
    "        self.BatchNorm3 = BatchNorm2d(Proc_Channel)\n",
    "    def forward(self, x):\n",
    "        Residual = self.shortcut(x)\n",
    "        x = self.ConvLayer1(x)\n",
    "        x = self.BatchNorm1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.ConvLayer2(x)\n",
    "        x = self.BatchNorm2(x)\n",
    "        x = F.relu(x)\n",
    "        x = x + Residual\n",
    "        x = self.BatchNorm3(x)\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64593901-c321-4834-8c03-961cb247efa5",
   "metadata": {},
   "source": [
    "![resnet.png](\"C:\\Users\\Baaaatttlllllllleeee\\Pictures\\resnet.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b421de33-3dd7-405e-836a-efdf84cd81fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self,DownSample,Proc_Channel):\n",
    "        super(Bottleneck,self).__init__()\n",
    "        Proc_Channel = int(Proc_Channel)\n",
    "        stride = 1\n",
    "        in_channels = int(Proc_Channel * 4)\n",
    "        if DownSample == 1:\n",
    "            if Proc_Channel == 64:\n",
    "                in_channels = Proc_Channel\n",
    "            else:\n",
    "                stride = 2\n",
    "                in_channels = int(Proc_Channel * 2)\n",
    "        self.ConvLayer1 = Sequential(Conv2d(in_channels, Proc_Channel, 1, 1, 0),\n",
    "                                     BatchNorm2d(Proc_Channel), \n",
    "                                     ReLU())\n",
    "        self.ConvLayer2 = Sequential(Conv2d(Proc_Channel, Proc_Channel, 3, stride, 1),\n",
    "                                     BatchNorm2d(Proc_Channel), \n",
    "                                     ReLU())\n",
    "        self.ConvLayer3 = Sequential(Conv2d(Proc_Channel, Proc_Channel * 4, 1, 1, 0),\n",
    "                                     BatchNorm2d(Proc_Channel * 4), \n",
    "                                     ReLU())\n",
    "        self.shortcut = Sequential(Conv2d(in_channels, Proc_Channel * 4, 3, stride, 1),\n",
    "                                   BatchNorm2d(Proc_Channel * 4))\n",
    "        self.Post = Sequential(BatchNorm2d(Proc_Channel * 4),\n",
    "                               ReLU())\n",
    "    def forward(self,x):\n",
    "        Residual = self.shortcut(x)\n",
    "        x = self.ConvLayer1(x)\n",
    "        x = self.ConvLayer2(x)\n",
    "        x = self.ConvLayer3(x)\n",
    "        x = x + Residual\n",
    "        x = self.Post(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba3f456-e759-48bb-9aae-8be16e1432af",
   "metadata": {},
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, DownSample, Proc_Channel):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        Proc_Channel = int(Proc_Channel / 4)\n",
    "        stride = 1\n",
    "        in_channels = Proc_Channel * 4\n",
    "        if DownSample == 1:\n",
    "            stride = 2\n",
    "            in_channels = int(Proc_Channel * 2)\n",
    "            if Proc_Channel == 64:\n",
    "                stride = 1\n",
    "                in_channels = Proc_Channel\n",
    "            self.shortcut = Sequential(Conv2d(in_channels = in_channels, out_channels = Proc_Channel * 4, kernel_size = 3, stride = stride, padding = 1),\n",
    "                                       BatchNorm2d(Proc_Channel * 4))\n",
    "        else:\n",
    "            self.shortcut = Sequential(Conv2d(in_channels = in_channels, out_channels = Proc_Channel * 4, kernel_size = 3, stride = stride, padding = 1),\n",
    "                                       BatchNorm2d(Proc_Channel * 4))\n",
    "        self.ConvLayer1 = Sequential(Conv2d(in_channels = in_channels, out_channels = Proc_Channel, kernel_size = 1, stride = 1, padding = 0),\n",
    "                                     BatchNorm2d(Proc_Channel),\n",
    "                                     ReLU())\n",
    "        self.ConvLayer2 = Sequential(Conv2d(in_channels = Proc_Channel, out_channels = Proc_Channel, kernel_size = 3, stride = stride, padding = 1),\n",
    "                                     BatchNorm2d(Proc_Channel),\n",
    "                                     ReLU())\n",
    "        self.ConvLayer3 = Sequential(Conv2d(in_channels = Proc_Channel, out_channels = Proc_Channel * 4, kernel_size = 1, stride = 1, padding = 0),\n",
    "                                     BatchNorm2d(Proc_Channel * 4))\n",
    "        self.Res_Proc = Sequential(BatchNorm2d(Proc_Channel * 4),\n",
    "                                   ReLU())\n",
    "    def forward(self, x):\n",
    "        Residual = self.shortcut(x)\n",
    "        x = self.ConvLayer1(x)\n",
    "        x = self.ConvLayer2(x)\n",
    "        x = self.ConvLayer3(x)\n",
    "        x = x + Residual\n",
    "        x = self.Res_Proc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78f560e6-bb31-4a1a-96d8-506e3f01b906",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layer_arch, final_output):\n",
    "        super(ResNet,self).__init__()\n",
    "        self.Resize = Resize((224,224), interpolation = InterpolationMode.BILINEAR)\n",
    "        self.final_output = final_output\n",
    "        self.stem = Sequential(Conv2d(in_channels = 3, out_channels = 64, kernel_size = 7, stride = 2, padding = 3),\n",
    "                               BatchNorm2d(64),\n",
    "                               ReLU(),\n",
    "                               MaxPool2d(kernel_size = 3, stride = 2, padding = 1))\n",
    "        self.stage1 = self.make_layer(block, 64, layer_arch[0])\n",
    "        self.stage2 = self.make_layer(block, 128, layer_arch[1])\n",
    "        self.stage3 = self.make_layer(block, 256, layer_arch[2])\n",
    "        self.stage4 = self.make_layer(block, 512, layer_arch[3])\n",
    "        self.Aver_pool = AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = Linear(self.final_output, 10)\n",
    "        self.softmax = Softmax(dim = 1)\n",
    "    def make_layer(self, block, Proc_Channel, layer_arch):\n",
    "        layer = []\n",
    "        for i in range(layer_arch):\n",
    "            if i == 0:\n",
    "                layer.append(block(1,Proc_Channel))\n",
    "            else:\n",
    "                layer.append(block(0,Proc_Channel))\n",
    "        return Sequential(*layer)\n",
    "    def forward(self,x):\n",
    "        x = self.Resize(x)\n",
    "        x = self.stem(x)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "        x = self.Aver_pool(x)\n",
    "        x = x.view(-1, self.final_output)\n",
    "        x = self.fc(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912d91eb-bdc6-4c70-8baa-4d8a349af4bf",
   "metadata": {},
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layer_arch, final_output): \n",
    "        super(ResNet, self).__init__()\n",
    "        self.final_output = final_output\n",
    "        self.Resize = Resize((224,224), interpolation = InterpolationMode.BILINEAR)\n",
    "        self.stem = Sequential(Conv2d(in_channels = 3, out_channels = 64, kernel_size = 7, stride = 2, padding =3),\n",
    "                               BatchNorm2d(64),\n",
    "                               ReLU(),\n",
    "                               MaxPool2d(kernel_size = 3, stride = 2, padding = 1))\n",
    "        self.stage1 = self._make_layer(block, layer_arch[0], int(self.final_output / 8))\n",
    "        self.stage2 = self._make_layer(block, layer_arch[1], int(self.final_output / 4))\n",
    "        self.stage3 = self._make_layer(block, layer_arch[2], int(self.final_output / 2))\n",
    "        self.stage4 = self._make_layer(block, layer_arch[3], self.final_output)\n",
    "        self.AvgPool = AdaptiveAvgPool2d((1,1))\n",
    "        self.fc1 = Linear(self.final_output, 1000)\n",
    "        self.fc2 = Linear(1000,10)\n",
    "        self.softmax = Softmax(dim = 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.Resize(x)\n",
    "        x = self.stem(x)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "        x = self.AvgPool(x)\n",
    "        x = x.view(-1,self.final_output)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def _make_layer(self, block, layer_arch, Proc_Channel):\n",
    "        layer = Sequential()\n",
    "        for i in range(layer_arch):\n",
    "            if i == 0:\n",
    "                layer.append(block(True, Proc_Channel))\n",
    "            else:\n",
    "                layer.append(block(False, Proc_Channel))\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bedef48f-27cb-47fe-ac52-815cf55426a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2,2,2,2], 512)\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3,4,6,3], 512)\n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3,4,6,3] , 2048)\n",
    "def ResNet101():\n",
    "    return ResNet(Bottleneck, [3,4,23,3] , 2048)\n",
    "def ResNet152():\n",
    "    return ResNet(Bottleneck, [3,8,36,3] , 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fed9fb90-09f4-43f4-8019-6c970a53d77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "Network = ResNet50()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "Network.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7b4671a-80b3-4318-926e-cb28bd7ab790",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = CrossEntropyLoss()\n",
    "#loss_function =  MSELoss() # Failed\n",
    "#loss_function = NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "845a968c-cefa-4736-a4d7-169312588efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_counter = []\n",
    "train_loss_counter = []\n",
    "train_data_counter = []\n",
    "test_acc_counter = []\n",
    "test_data_counter = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6659bf0f-7c34-4759-927a-99bb65bb11dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    Network.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        for data, target in Test_Data:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output = Network(data).to(device)\n",
    "            output = output.data.max(1, keepdim = True)[1]\n",
    "            correct += output.eq(target.data.view_as(output)).sum()\n",
    "        log = \"Mode: Test | Epoch:{} Memory:{:.0f}MB Acc:{:.0f}%\".format(epoch,  memory_allocated(device)/1048576, 100. * correct/len(Test_Data.dataset))\n",
    "        writer.add_scalar(tag = 'TestAccuracy', scalar_value=(100. * correct/len(Test_Data.dataset)), global_step=(epoch * len(Train_Data.dataset)))\n",
    "        print(log)\n",
    "        test_acc_counter.append((100. * correct/len(Test_Data.dataset)))\n",
    "        test_data_counter.append(epoch * len(Train_Data.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bafe5ee-946d-4c43-bb05-62a35b93706d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs):\n",
    "    for epoch in range(1, epochs+1):\n",
    "        Network.train()\n",
    "        empty_cache()\n",
    "        for batch_idx, (data, target) in enumerate(Train_Data):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            correct = 0\n",
    "            output = Network(data)\n",
    "            pred = output.data.max(1, keepdim = True)[1]\n",
    "            pred = pred.view_as(target)\n",
    "            correct += pred.eq(target.data).sum()\n",
    "            correct = int(correct)\n",
    "            loss = loss_function(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % log_interval == 0 and batch_idx != 0:\n",
    "                log = \"Mode: Train | Epoch:{} Data:{}/{} Memory:{:.0f}MB Accuracy:{:.0f}% Loss:{:.6f}\".format(epoch, batch_idx*len(data), len(Train_Data.dataset), memory_allocated(device)/1048576, 100. * (correct/len(data)), loss.item())\n",
    "                print(log)\n",
    "                train_acc_counter.append((100. * (correct/len(data))))\n",
    "                train_loss_counter.append(loss.item())\n",
    "                train_data_counter.append((epoch-1) * len(Train_Data.dataset) + batch_idx * len(data))\n",
    "                writer.add_scalar(tag = 'TrainAccuracy', scalar_value=(100. * (correct/len(data))), global_step=(epoch-1) * len(Train_Data.dataset) + batch_idx * len(data))\n",
    "                writer.add_scalar(tag = 'TrainLoss', scalar_value=loss.item(), global_step=(epoch-1) * len(Train_Data.dataset) + batch_idx * len(data))\n",
    "        empty_cache()\n",
    "        test(epoch)\n",
    "        scheduler.step()\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b32e68dc-cbec-4877-a34c-8121e60d1ff7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(params = Network.parameters(), lr = lr) \n",
    "#optimizer = SGD(params = Network.parameters(), lr = lr, momentum = momentum)\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f425640c-391a-4760-9794-63565c24fd9e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 6.00 GiB total capacity; 3.72 GiB already allocated; 0 bytes free; 4.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\BAAAAT~1\\AppData\\Local\\Temp/ipykernel_14752/218368184.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\BAAAAT~1\\AppData\\Local\\Temp/ipykernel_14752/3149854692.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epochs)\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mcorrect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m             )\n\u001b[1;32m--> 488\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 6.00 GiB total capacity; 3.72 GiB already allocated; 0 bytes free; 4.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "train(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfceebe-b1df-475d-a454-6e427c2f052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(memory_allocated(device)/1048576)\n",
    "print(memory_reserved(device)/1048576)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660232f9-4c12-4ec6-9ac1-40b1d99184b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d3b456-a483-4ce4-93b9-84e1f2c1dda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Network.to('cpu')\n",
    "empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3981259b-8e4b-4ec6-bd5e-fec0edd12c43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
